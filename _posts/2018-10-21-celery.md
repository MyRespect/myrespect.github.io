---
layout: post
title:  "Celery Distributed Task Queue"
categories: Distributed_System
tags: Celery RabbitMQ
--- 

* content
{:toc}

Celery is a distributed system to process vast amounts of messages, while providing operations with the tools requires to maintain such a system. It is a task queue with focus on real-time processing, while also supporting task scheduling.




Celery的架构由三部分组成，消息中间件（message broker），任务执行单元（worker）和任务执行结果存储（task result store）组成．在具体操作celery之前，我们先来介绍几个概念．

#### **Producer-Consumer**

生产者，负责产生数据的模块；消费者，负责处理数据的模块；中介，作为一个缓冲区处于生产者和消费者之间．生产者把数据放入到缓冲区，消费者从缓冲区取出数据；这个缓冲区起到了＂解耦＂的作用;

在异步通讯中，消息不会立刻到达接收方，而是被存放在一个容器中，当满足一定条件之后，消息会被容器发送给接收方，这个容器即消息队列. 完成这个功能需要双方和容器以及其中的各个组件遵守统一的约定和规则, AMQP (advanced message queuing protocol)就是这样一种协议，RabbitMQ就是基于AMQP的消息队列产品;

AMQP包含的主要元素,　producer发布消息; consumer从消息队列中获取消息; message queue消息队列用于保存消息; exchange(交换器)，路由组件接受producer发送的消息并将消息路由转发给消息队列; 虚拟主机是一批交换器，消息队列和相关对象的集合;信道(channel)是多路复用连接中的一条独立的双向数据流通道，为回话提供物理传输介质．

```
publisher--publish-->exchange--routes-->queue--consume-->consumer
```

#### **Celery with Django**

在[Celery](http://docs.celeryproject.org/en/latest/)的网站,有比较清楚的入门文档介绍．入门比较简单，但想要用好它还需要多思考多实践．

```
celery -A petct worker -l info --pidfile id.txt 
# The --pidfile argument can be set to an absolute path to avoid  having other old workers still running.
```

##### **Add Task**
```
# tasks.py

from __future__ import absolute_import, unicode_literals
from celery import shared_task
from celery.decorators import task
from celery.utils.log import get_task_logger
from .draw3d import loadArray

logger = get_task_logger(__name__)

@shared_task(name="loadArray")
def loadArray_task(filepath, shape):
    logger.info("load 3d array")
    return loadArray(filepath, shape)



# views.py

from .tasks import loadArray_task

def show(request):
    if request.method == 'GET':
        filePos=request.GET.get("userID", None)
        record=Record.objects.get(userID=filePos)

        global mydict
        if filePos not in mydict:
            loadtask=loadArray_task.delay("./file.mpv", (110,110,187))
            mydict[filePos]=loadtask.get(timeout=1)

        return render(request, 'show.html', {'record': record})
```

##### **Periodic Tasks**

Scheduling a task to run at a specific time, [Refer](https://realpython.com/asynchronous-tasks-with-django-and-celery/).

```
# tasks.py
# we run the save_latest_flickr_image() function every fifteen minutes by wrapping the function call in a task. 

from celery.task.schedules import crontab
from celery.decorators import periodic_task
from celery.utils.log import get_task_logger

from photos.utils import save_latest_flickr_image

logger = get_task_logger(__name__)

@periodic_task(
    run_every=(crontab(minute='*/15')),
    name="task_save_latest_flickr_image",
    ignore_result=True
)
def task_save_latest_flickr_image():
    """
    Saves latest image from Flickr
    """
    save_latest_flickr_image()
    logger.info("Saved image from Flickr")
```

We can also run the job remotely using **Supervisor**. We need to tell Supervisor about our Celery workers by adding configuration files to the “/etc/supervisor/conf.d/” directory on the remote server. In our case, we need two such configuration files - one for the Celery worker and one for the Celery scheduler.